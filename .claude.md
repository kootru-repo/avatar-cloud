# Gemini Live Avatar - Claude Code Project Instructions

## Project Overview

Voice-to-voice AI avatar application using Google's Gemini 2.5 Flash Live API with native audio and synchronized video playback.

**Tech Stack:**
- Frontend: Vanilla JavaScript, Material Design 3, WebSocket client
- Backend: Python 3.13, asyncio, websockets
- AI: Gemini 2.5 Flash Live (09-2025) with native audio streaming
- Video: State-based avatar (idle, listening, speaking)

---

## üî¥ CRITICAL: ALWAYS USE THIS MODEL

**MODEL NAME (NEVER CHANGE):**
```
models/gemini-2.5-flash-native-audio-preview-09-2025
```

**API TYPE (NEVER CHANGE):**
```
Google AI Developer API (NOT Vertex AI)
Authentication: API Key (NOT Service Account)
API Version: v1alpha
```

**This applies to:**
- All configuration files (.env, config.json)
- All backend code (gemini_client.py, config files)
- All frontend code (gemini-api.js, config.json)
- All test scripts
- All documentation

**No exceptions. No variations. Always this exact model name and API.**

**Critical Details:**
- `models/` prefix REQUIRED for Google AI Developer API (per official Google code)
- Word order: `native-audio-preview` NOT `preview-native-audio`
- For Vertex AI the model is `gemini-live-2.5-flash-preview-native-audio-09-2025` (different!)
- We use Google AI Developer API for simplicity (API key vs service account)

---

## ‚ö†Ô∏è IMPORTANT: Documentation Policy

**DO NOT create summary documents, reports, or wrap-up files for:**
- Code fixes or changes made
- Problems solved
- Issues resolved
- Security patches applied
- Bug fixes completed
- Refactoring work
- Code cleanup
- Testing results

**Examples of documents NOT to create:**
- ‚ùå "FIXES_IMPLEMENTED.md"
- ‚ùå "CHANGES_SUMMARY.md"
- ‚ùå "WORK_COMPLETED.md"
- ‚ùå "ISSUE_RESOLUTION_REPORT.md"
- ‚ùå "CLEANUP_SUMMARY.md"
- ‚ùå Any similar wrap-up documentation

**Why:** The user prefers to track work through git commits and doesn't want summary documents cluttering the project.

**What TO create:**
- ‚úÖ Code files (.js, .py, .html, .css, .json)
- ‚úÖ Configuration files (.env.example, config.json)
- ‚úÖ Test files (test_*.py, *_test.js)
- ‚úÖ Essential documentation (README.md updates if critical)

---

## ‚ö†Ô∏è CRITICAL: SDK Usage Policy

**ONLY use official Google Generative AI SDK methods and patterns.**

### Before Planning ANY Code Change:
1. **ALWAYS check latest SDK references first** (documentation, examples, source code)
2. Verify the method/pattern is officially supported
3. Do not invent custom protocols or workarounds
4. Follow exact SDK patterns from official examples

### Key Findings:
- **Audio transmission:** Reference project sends audio with `end_of_turn=True` immediately
- **Turn signals:** Reference project ignores the "end" signal completely
- **DO NOT** create custom turn-end detection logic if SDK doesn't use it
- **DO NOT** assume SDK features exist without verification

### SDK Reference Sources:

**‚ö†Ô∏è CRITICAL: Two Different APIs with Different Models**

This project uses **Vertex AI** (not Google AI Developer API). The documentation differs significantly:

| Aspect | Vertex AI | Google AI Developer API |
|--------|-----------|-------------------------|
| **Authentication** | Service Account + ADC | API Key |
| **SDK Package** | `google-genai` v1.51.0 | `google-genai` v1.51.0 (same package!) |
| **Live Models** | `gemini-2.0-flash-live-preview-04-09` | `gemini-live-2.5-flash-preview-native-audio-09-2025` |
| **Documentation** | https://googleapis.github.io/python-genai/ | https://googleapis.github.io/python-genai/ |
| **Parameter Style** | **snake_case** (ALWAYS) | **snake_case** (ALWAYS) |
| **MIME Type** | `audio/pcm` OR `audio/pcm;rate=16000` (both work!) | `audio/pcm` OR `audio/pcm;rate=16000` (both work!) |

**Vertex AI SDK Documentation (Use This!):**
- Primary Docs: https://googleapis.github.io/python-genai/
- GitHub: https://github.com/googleapis/python-genai
- PyPI: https://pypi.org/project/google-genai/
- Version: 1.51.0

**Google AI Developer API Docs (Reference Only):**
- Live API: https://ai.google.dev/gemini-api/docs/live
- Python SDK: https://ai.google.dev/gemini-api/docs/python-sdk
- Examples: https://github.com/google-gemini/multimodal-live-api-web-console

**Model Information (Vertex AI Live API):**
- **ONLY Supported Model**: `gemini-2.0-flash-live-preview-04-09` ‚úÖ
- Capabilities: Native audio streaming, multimodal input/output, low latency
- Audio Format: 16kHz PCM16 input, 24kHz PCM16 output

**Models That DON'T Work (Tested 2025-11-18):**
- ‚ùå `gemini-3-pro-preview` - Does not exist in Vertex AI
- ‚ùå `gemini-2.5-flash-native-audio-preview-09-2025` - Google AI Developer API only (not Vertex AI)
- ‚ùå `gemini-2.0-flash-exp` - Exists in Vertex AI but does NOT support Live API (WebSocket error: "not supported in the live api")

**CRITICAL**: Only `gemini-2.0-flash-live-preview-04-09` works with Vertex AI Live API. All other models either don't exist or don't support the Live API.

**If unsure whether a method is official:** Stop and verify against **Vertex AI docs** (googleapis.github.io/python-genai/), not ai.google.dev.

---

## Code Style & Conventions

### JavaScript
- No semicolons (project uses semicolons - follow existing style)
- Use `async/await` for asynchronous operations
- Use `const` and `let`, avoid `var`
- Functions are defined with `function` keyword (not arrow functions for named functions)
- Class-based architecture for managers (AvatarManager, MetricsManager, etc.)

### Python
- Type hints on function signatures
- `async/await` for all WebSocket operations
- f-strings for string formatting
- Descriptive logging with emojis for user-facing messages

### File Organization
```
frontend/
  ‚îú‚îÄ‚îÄ script.js                    # Main orchestration
  ‚îú‚îÄ‚îÄ avatar-manager.js            # Avatar video state management
  ‚îú‚îÄ‚îÄ gemini-live-api.js          # Gemini API client
  ‚îú‚îÄ‚îÄ live-media-manager.js       # Audio/video capture
  ‚îú‚îÄ‚îÄ config-loader.js            # Configuration loader
  ‚îú‚îÄ‚îÄ secure-token-storage.js     # OAuth token storage
  ‚îú‚îÄ‚îÄ *-manager.js                # Other managers
  ‚îî‚îÄ‚îÄ config.json                  # All configuration

backend/
  ‚îî‚îÄ‚îÄ main.py                      # WebSocket proxy server
```

---

## Security Guidelines

### OAuth Tokens
- **NEVER log tokens or token fragments**
- Use `[REDACTED]` in all logs
- Store in `sessionStorage` (not cookies)
- Validate format (`ya29.` prefix)
- Always use `SecureTokenStorage` class

### Input Validation
- Validate all WebSocket messages
- Check message size (1MB max)
- Validate token format before use
- Rate limit: 10 connections/minute per IP
- CORS: Validate origin headers

### Security Checklist
- [ ] No tokens in logs
- [ ] No sensitive data in error messages
- [ ] Use `textContent` not `innerHTML`
- [ ] Validate all user inputs
- [ ] Rate limiting enabled
- [ ] CORS configured

---

## Configuration

All settings centralized in `frontend/config.json`:
- API endpoints and credentials
- Video sources and timing
- Audio settings
- Feature flags
- UI preferences
- Voice and affective dialog settings

**To change settings:** Edit `config.json`, never hardcode values.

### Key Configuration Options:

**Voice Settings** (`geminiVoice`):
```json
"geminiVoice": {
  "enabled": true,
  "voiceName": "Orion",
  "affectiveDialog": true
}
```
- **voiceName**: Choose from Puck, Charon, Kore, Fenrir, Aoede, Zubenelgenubi, Orion, Pegasus, Vega
- **affectiveDialog**: When enabled, Gemini adapts its response style to match input expression and tone (requires API v1alpha)

**Speaking Video Cycle** (`speakingCycle`):
```json
"speakingCycle": {
  "enabled": true,
  "initialForwardDuration": 3.0,
  "reverseDuration": 2.0,
  "forwardDuration": 2.0
}
```
- Creates dynamic talking animation: plays forward for N seconds, then cycles reverse/forward

---

## Key Modules

### Avatar Video System
- **AvatarManager** (`avatar-manager.js`): State-based video playback
- States: `idle`, `listening`, `speaking`
- Micro-palindrome mode for smooth transitions

### WebSocket Flow
1. Frontend ‚Üí Backend (port 8080)
2. Backend ‚Üí Gemini API (with OAuth token)
3. Bidirectional streaming
4. Backend = secure proxy (adds auth, rate limiting)

### Audio Pipeline
- Input: 16kHz PCM16 ‚Üí Gemini 2.5 Flash Live
- Output: 24kHz PCM16 ‚Üê Gemini 2.5 Flash Live
- Native audio streaming (low latency, high quality)
- AudioWorklet for processing

---

## Testing

### Manual Testing
```bash
# Start backend
python backend/main.py

# Start frontend (separate terminal)
cd frontend && python -m http.server 8000

# Or use quick start
START.bat
```

### Test Checklist
- [ ] Backend starts without errors
- [ ] Frontend loads, no console errors
- [ ] Avatar video shows idle state
- [ ] Connection flow works
- [ ] Audio input/output functional
- [ ] No token leaks in logs

---

## Common Tasks

### Adding a New Feature
1. Check if config option needed ‚Üí add to `config.json`
2. Create manager class if complex (e.g., `FeatureManager`)
3. Initialize in `initializeConfigDependentComponents()`
4. Wire up UI in `index.html` and event handlers in `script.js`
5. Test thoroughly, no summary docs

### Fixing a Bug
1. Identify root cause
2. Fix the code
3. Test the fix
4. Commit with clear message
5. **DO NOT create fix summary document**

### Adding Configuration
1. Add to `config.json` with sensible default
2. Access via `AppConfig.get('path.to.setting', defaultValue)`
3. Document in code comments if complex

---

## üö® CRITICAL DEPLOYMENT WARNING - READ THIS FIRST

### FIREBASE_PROJECT_ID Environment Variable

**‚ö†Ô∏è THIS IS THE #1 RECURRING DEPLOYMENT FAILURE ISSUE ‚ö†Ô∏è**

**PROBLEM:** After backend deployment, the `FIREBASE_PROJECT_ID` environment variable is often missing, causing immediate connection failures with error:
```
‚ùå Blocked connection from unauthorized origin: https://avatar-478217.web.app
   Allowed origins: (empty)
```

**WHY IT HAPPENS:** Cloud Run deployments sometimes don't preserve environment variables if not explicitly included in the deploy command.

**SOLUTION - ALWAYS INCLUDE IN BACKEND DEPLOYS:**

```bash
cd backend
gcloud run deploy gemini-avatar-backend --source . --region us-central1 --allow-unauthenticated --min-instances 1 --max-instances 10 --timeout 300 --memory 512Mi --cpu 1 --set-env-vars BACKEND_HOST=0.0.0.0,BACKEND_PORT=8080,DEBUG=false,REQUIRE_AUTH=false,FIREBASE_PROJECT_ID=avatar-478217 --set-secrets GEMINI_API_KEY=GEMINI_API_KEY:latest --port 8080
```

**KEY POINT:** The `FIREBASE_PROJECT_ID=avatar-478217` MUST be in EVERY backend deployment command.

**AFTER DEPLOYMENT, ALWAYS VERIFY:**
```bash
# Check environment variables are set
gcloud run services describe gemini-avatar-backend \
  --region us-central1 \
  --format="value(spec.template.spec.containers[0].env)" | grep FIREBASE

# Should show: {'name': 'FIREBASE_PROJECT_ID', 'value': 'avatar-478217'}
```

**IF CONNECTION FAILS IMMEDIATELY AFTER DEPLOYMENT:**
```bash
# Fix by adding the missing environment variable
gcloud run services update gemini-avatar-backend \
  --region us-central1 \
  --set-env-vars FIREBASE_PROJECT_ID=avatar-478217
```

**REMEMBER:** This happens EVERY TIME. Check the environment variable EVERY TIME.

---

## Deployment Workflow

### Automatic Deployment After Code Changes

**CRITICAL: Always deploy changes immediately after modifying code.**

#### Frontend Deployment (Firebase Hosting)

When frontend files are changed (`frontend/index.html`, `frontend/app.js`, `frontend/*.js`, etc.):

1. **Attempt deployment:**
   ```bash
   firebase deploy --only hosting
   ```

2. **If authentication fails:**
   ```bash
   firebase login --reauth
   ```
   - This will open a browser for user authentication
   - **WAIT for user to complete the browser authentication**
   - User will see "Firebase CLI Login Successful" in browser
   - Then retry deployment:
   ```bash
   firebase deploy --only hosting
   ```

3. **Verify deployment:**
   - Check output for deployment URL
   - Should show: `‚úî  Deploy complete!`
   - Frontend URL: https://avatar-478217.web.app

#### Backend Deployment (Cloud Run)

When backend files are changed (`backend/main.py`, `backend/core/*.py`, `backend/requirements.txt`, etc.):

1. **Deploy to Cloud Run:**
   ```bash
   cd backend
   gcloud run deploy gemini-avatar-backend --source . --region us-central1 --allow-unauthenticated --min-instances 1 --max-instances 10 --timeout 300 --memory 512Mi --cpu 1 --set-env-vars BACKEND_HOST=0.0.0.0,BACKEND_PORT=8080,DEBUG=false,REQUIRE_AUTH=false,FIREBASE_PROJECT_ID=avatar-478217 --set-secrets GEMINI_API_KEY=GEMINI_API_KEY:latest --port 8080
   ```

2. **If authentication fails:**
   - Authentication errors in non-interactive mode cannot be resolved by Claude Code
   - **Inform user to run the deployment command manually in their terminal**
   - Provide the exact command above for them to copy/paste

3. **Verify deployment:**
   - Check output for service URL
   - Should show: `Service [gemini-avatar-backend] revision [...] has been deployed`
   - Backend URL: https://gemini-avatar-backend-580499038386.us-central1.run.app

#### Deployment Decision Tree

```
Code Changed?
‚îÇ
‚îú‚îÄ‚îÄ frontend/* ‚Üí Run firebase deploy --only hosting
‚îÇ                If auth fails ‚Üí firebase login --reauth ‚Üí wait for user ‚Üí retry
‚îÇ
‚îú‚îÄ‚îÄ backend/*  ‚Üí Run gcloud run deploy gemini-avatar-backend ...
‚îÇ                If auth fails ‚Üí inform user to run command manually
‚îÇ
‚îî‚îÄ‚îÄ Both       ‚Üí Deploy backend first, then frontend
```

#### Key Deployment Notes

- **Always commit to git first:** Changes should be version controlled before deployment
- **Firebase auth is interactive:** Claude Code can trigger `firebase login --reauth` and wait for user
- **gcloud auth is manual:** Non-interactive mode prevents automatic reauth, user must deploy manually
- **Verify each deployment:** Check for success messages and service URLs
- **Test after deployment:** Verify changes are live at production URLs

#### Environment Variables (Backend)

Required environment variables for Cloud Run deployment:
```bash
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8080
DEBUG=false
REQUIRE_AUTH=true
FIREBASE_PROJECT_ID=avatar-478217  # Allows frontend origin
```

Required secrets:
```bash
GEMINI_API_KEY  # From Secret Manager (latest version)
```

#### Deployment Checklist

After any code change:
- [ ] Git commit completed
- [ ] Appropriate deployment command run
- [ ] Authentication successful (or user notified)
- [ ] Deployment completed without errors
- [ ] Service URL verified
- [ ] Changes tested at production URL

---

## Debugging

### Backend Issues
- Check logs for `‚ùå` markers
- Verify port 8080 not in use: `netstat -ano | findstr :8080`
- Token format: Must start with `ya29.`
- Check `ALLOWED_ORIGINS` environment variable

### Frontend Issues
- Open DevTools Console (F12)
- Look for connection errors
- Verify `config.json` loads successfully
- Check Network tab for WebSocket status

### Avatar Video Issues
- Check browser console for video load errors
- Ensure video paths in `config.json` are correct

---

## Git Workflow

### Commit Messages
```
Format: <type>: <description>

Types:
  feat: New feature
  fix: Bug fix
  refactor: Code refactoring
  perf: Performance improvement
  security: Security fix
  chore: Maintenance task
  docs: Documentation only

Example:
  security: remove token logging from backend
  feat: add micro-palindrome avatar transitions
  fix: resolve WebSocket reconnection issue
```

### What to Commit
- ‚úÖ Code changes
- ‚úÖ Configuration updates
- ‚úÖ Test files
- ‚ùå Summary documents or reports
- ‚ùå Temporary diagnostic files
- ‚ùå Personal notes

---

## File Naming Conventions

- Configuration: `config.json`, `.env`
- Managers: `*-manager.js` (kebab-case)
- Tests: `test_*.py`, `*_test.js`
- Batch files: `UPPERCASE.bat`
- Documentation: `UPPERCASE.md` (only essential docs)

---

## Dependencies

### Backend (Python)
```
websockets>=12.0
```

### Frontend (JavaScript)
- Material Design 3 Web Components (CDN)
- No build step, runs in browser

---

## Environment Variables

```bash
# Backend
BACKEND_PORT=8080
BACKEND_HOST=0.0.0.0
DEBUG=false
ALLOWED_ORIGINS=http://localhost:8000,http://localhost:8080

# Not used - tokens from user input
# PROJECT_ID in config.json
```

---

## Video Files

Located in `media/video/`:

**Customization:** Update paths in `config.json` ‚Üí `video.sources`

---

## Project Specific Notes

### Why No Summary Docs?
- User tracks work via git commits
- Reduces file clutter
- Focus on code, not meta-documentation
- Clear git history is sufficient

### When Documentation IS Needed
- Critical security vulnerability discovered
- Major architecture change affecting multiple systems
- Breaking API changes
- User specifically requests documentation

---

## Quick Reference

### Start Development
```bash
START.bat  # Easiest - auto-starts everything
```

### Get OAuth Token
```bash
gcloud auth print-access-token
```

### Check Backend Status
```bash
netstat -ano | findstr :8080
```

### View Logs
- Backend: Terminal where `python backend/main.py` runs
- Frontend: Browser DevTools Console (F12)

---

## Remember

1. **Never create summary documents** for routine work
2. **Always validate OAuth tokens** before use
3. **Use config.json** for all settings
4. **Test thoroughly** after changes
5. **Keep it simple** - don't over-engineer
6. **Security first** - no token logging ever

---

**Project Status:** 100% SDK Compliant - Complete Overhaul (Nov 2025)
**Primary Goal:** Pure SDK implementation of Gemini Live Avatar

---

## üÜï SDK Compliance Reference (Nov 2025)

### Official Google Gen AI SDK
- **Documentation**: https://googleapis.github.io/python-genai/
- **GitHub**: https://github.com/googleapis/python-genai
- **PyPI**: https://pypi.org/project/google-genai/
- **Version**: 1.51.0 (Latest as of Nov 18, 2025)

### Critical SDK Method Signatures

**send_realtime_input** - for audio/video:
```python
await session.send_realtime_input(
    audio=types.Blob(data=bytes, mime_type="audio/pcm")  # Named parameter!
)
```

**send_client_content** - for text/images:
```python
await session.send_client_content(
    turns=types.Content(role="user", parts=[...]),  # turns= not content=!
    turn_complete=True  # turn_complete= not end_of_turn=!
)
```

**send_tool_response** - for function results:
```python
await session.send_tool_response(
    function_responses=[...]  # function_responses= parameter!
)
```

### SDK Configuration Pattern (OFFICIAL)
```python
# CORRECT: Always use snake_case for ALL parameters
config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],        # snake_case
    speech_config=types.SpeechConfig(     # snake_case
        voice_config=types.VoiceConfig(   # snake_case
            prebuilt_voice_config=types.PrebuiltVoiceConfig(  # snake_case
                voice_name="Puck"
            )
        )
    ),
    system_instruction=types.Content(     # snake_case
        role="user",
        parts=[types.Part(text="...")]
    )
)
```

**Official Reference**: https://googleapis.github.io/python-genai/

### SDK Warnings
‚ö†Ô∏è **Do not interleave `send_client_content` and `send_realtime_input` in the same turn** - can cause unexpected results

---

## üéØ Function Calling Feature Template: Dance Mode Implementation

This section documents the complete implementation of the dance mode feature using Gemini's Function Calling (tool calling) capability. Use this as a template for implementing similar interactive features.

### Architecture Overview

**Trigger Flow:**
```
User says "dance"
  ‚Üí Gemini hears via audio input
  ‚Üí Gemini calls trigger_dance_mode() function
  ‚Üí Backend receives tool_call from Gemini
  ‚Üí Backend forwards to frontend via WebSocket
  ‚Üí Backend sends tool_response back to Gemini
  ‚Üí Frontend triggers dance animation + music
  ‚Üí Gemini continues speaking enthusiastically
  ‚Üí After 10.084s, dance ends automatically
```

**Key Design Decisions:**
1. **Server-side detection only** - No client-side keyword matching (prevents dual triggering)
2. **Audio coexistence** - Dance music plays at 30% volume while Gemini speaks
3. **User can interrupt** - Recording stays active during dance
4. **Exact timing** - Music duration matches video duration precisely with 1s fade out

---

### Step 1: Define the Function in Backend Config

**File:** `backend/config/gemini_config.py`

```python
# FUNCTION CALLING: Define tools that Gemini can call
config["tools"] = [
    {
        "function_declarations": [
            {
                "name": "trigger_dance_mode",
                "description": "Triggers the avatar's dance mode, playing music and showing a dance animation for 10 seconds. IMPORTANT: You MUST continue speaking enthusiastically about dancing while calling this function - the dance music plays quietly in the background so the user can still hear you. Use this when the user asks to dance, mentions dancing, or requests dance music. Parameters are empty (no configuration needed).",
                "parameters": {
                    "type": "OBJECT",
                    "properties": {},
                    "required": []
                }
            }
        ]
    }
]
```

**Critical Details:**
- Description explicitly instructs Gemini to continue speaking
- Explains that music is quiet background (prevents silence)
- Parameters are empty but structure must be valid OBJECT type
- This gets passed to Gemini session via `client.aio.live.connect(model=MODEL, config=config)`

---

### Step 2: Update Character Backstory

**File:** `backend/whinny_backstory.json`

```json
{
  "reaction_imperatives": {
    "dance_command": {
      "triggers": ["dance", "dancing", "dance music", "let's dance", "show me some moves", "bust a move"],
      "response": "Use the trigger_dance_mode function AND speak enthusiastically",
      "rule": "When user mentions dancing or asks to dance, you MUST do TWO things simultaneously: (1) call the trigger_dance_mode() function to start the visual dance sequence, AND (2) speak enthusiastically about dancing with high energy and excitement. The function call and your speech happen at the same time - don't go silent!"
    }
  }
}
```

**Critical Details:**
- Rule uses "MUST" not "can" - forces simultaneous action
- Explicitly states "don't go silent" to prevent audio dropout
- Lists all trigger phrases for reference (Gemini learns from context)

---

### Step 3: Implement Backend Tool Handler

**File:** `backend/core/websocket_handler.py`

```python
# SDK-COMPLIANT: Handle tool_call (function calling)
if hasattr(response, 'tool_call') and response.tool_call:
    logger.info(f"üîß Tool call received: {response.tool_call}")

    # SDK structure: tool_call.function_calls is an array (plural!)
    for function_call in response.tool_call.function_calls:
        logger.info(f"   Calling function: {function_call.name} (id={function_call.id})")

        # Debounce rapid duplicate calls (2 second cooldown)
        from datetime import datetime, timedelta
        now = datetime.now()
        should_skip = False
        if (session.last_tool_call_name == function_call.name and
            session.last_tool_call_time and
            (now - session.last_tool_call_time).total_seconds() < 2.0):
            logger.warning(f"   ‚ö†Ô∏è Skipping duplicate tool call within cooldown")
            should_skip = True
        else:
            session.last_tool_call_name = function_call.name
            session.last_tool_call_time = now

        if should_skip:
            # Still acknowledge to prevent Gemini waiting
            tool_response = types.LiveClientToolResponse(
                function_responses=[
                    types.FunctionResponse(
                        id=function_call.id,
                        name=function_call.name,
                        response={"success": False, "reason": "Duplicate call within cooldown"}
                    )
                ]
            )
            await session.genai_session.send(tool_response)
            continue

        # Forward to frontend with error handling
        try:
            await websocket.send(json.dumps({
                "type": "tool_call",
                "data": {
                    "name": function_call.name,
                    "args": function_call.args,
                    "id": function_call.id,
                    "timestamp": now.isoformat()
                }
            }))
            logger.info(f"   ‚úÖ Tool call forwarded to frontend")
        except Exception as e:
            logger.error(f"   ‚ùå Failed to forward tool call: {e}")

        # Send tool_response back to Gemini (acknowledges execution)
        tool_response = types.LiveClientToolResponse(
            function_responses=[
                types.FunctionResponse(
                    id=function_call.id,
                    name=function_call.name,
                    response={"success": True}
                )
            ]
        )
        await session.genai_session.send(tool_response)
        logger.info(f"   ‚úÖ Tool response sent to Gemini (id={function_call.id})")
```

**Critical Details:**
- `tool_call.function_calls` is PLURAL (array) - common mistake
- Debouncing prevents spam if user says "dance dance dance"
- Error handling prevents WebSocket failures from breaking flow
- `tool_response` MUST be sent back to Gemini or it waits indefinitely
- Timestamp added for debugging timing issues

**Session State Addition:**
Add to `backend/core/session.py`:
```python
@dataclass
class SessionState:
    # ... existing fields ...

    # Tool call debouncing
    last_tool_call_name: Optional[str] = None
    last_tool_call_time: Optional[datetime] = None
```

---

### Step 4: Implement Frontend Tool Handler

**File:** `frontend/app.js`

```javascript
case 'tool_call':
    // Log with ID for debugging
    const toolId = message.data.id || 'no-id';
    const toolName = message.data.name;
    console.log(`üîß Tool call received: ${toolName} (id: ${toolId})`, message.data);
    this.log(`üîß Tool call: ${toolName}`, 'info');

    // Handle dance mode tool call
    if (toolName === 'trigger_dance_mode') {
        console.log(`üéØ Dance mode tool called by Gemini! (id: ${toolId})`);
        this.triggerDanceMode();
    }
    break;
```

**Dance Mode Implementation:**
```javascript
triggerDanceMode() {
    // Validation
    if (!this.danceModeConfig || !this.danceModeConfig.enabled) {
        this.log('‚ùå Dance mode disabled', 'error');
        return;
    }

    // Validate duration (max 60 seconds)
    const MAX_DANCE_DURATION = 60000;
    if (!this.danceModeConfig.duration ||
        this.danceModeConfig.duration <= 0 ||
        this.danceModeConfig.duration > MAX_DANCE_DURATION) {
        this.log('‚ùå Invalid dance duration', 'error');
        return;
    }

    if (this.isDancing) {
        console.log('‚ö†Ô∏è Already dancing!');
        return;
    }

    this.log('üíÉ Dance mode activated!', 'info');
    this.isDancing = true;

    // DON'T stop audio - let Gemini speak!
    // DON'T stop recording - allow user to interrupt!
    const wasRecording = this.isRecording;

    // Switch to dancing video
    this.setAvatarState('dancing');

    // Load music with environment awareness
    let musicPath = this.danceModeConfig.musicFile;
    if (typeof musicPath === 'object') {
        const isLocal = this.isLocalEnvironment();
        musicPath = isLocal ? musicPath.local : musicPath.cloud;
    }

    // Use preloaded music if available
    const usePreloaded = this.preloadedDanceMusic &&
                        (this.preloadedDanceMusic.src.endsWith(musicPath) ||
                         this.preloadedDanceMusic.src === musicPath);

    if (usePreloaded) {
        this.danceAudio = this.preloadedDanceMusic;
        this.danceAudio.currentTime = 0;
        if (!this.danceAudio.crossOrigin) {
            this.danceAudio.crossOrigin = "anonymous";
        }
    } else {
        this.danceAudio = new Audio(musicPath);
        this.danceAudio.crossOrigin = "anonymous";
        this.danceAudio.preload = "auto";
    }

    // Set volume from config (default 0.3)
    const volume = this.danceModeConfig.volume || 0.3;
    this.danceAudio.volume = Math.max(0, Math.min(1, volume));

    // Error handling
    this.danceAudio.addEventListener('error', (e) => {
        const errorMsg = `‚ùå Dance music failed to load`;
        console.error(errorMsg, this.danceAudio.error);
        this.log(errorMsg, 'error');
        this.stopDanceMode(false);
    });

    // Play music
    this.danceAudio.load();
    this.danceAudio.play().catch(err => {
        console.error('‚ùå Dance music play failed:', err);
        this.stopDanceMode(false);
    });

    // Auto-stop after duration
    this.danceTimeout = setTimeout(() => {
        this.stopDanceMode(wasRecording);
    }, this.danceModeConfig.duration);
}

stopDanceMode(resumeRecording = true) {
    if (!this.isDancing) return;

    console.log('üõë Stopping dance mode');
    this.isDancing = false;

    // Clear timeout
    if (this.danceTimeout) {
        clearTimeout(this.danceTimeout);
        this.danceTimeout = null;
    }

    // Stop music
    if (this.danceAudio) {
        this.danceAudio.pause();
        this.danceAudio.currentTime = 0;
        this.danceAudio = null;
    }

    // Return to listening state
    this.setAvatarState('listening');

    // Resume recording if it was active
    if (resumeRecording && this.audioRecorder) {
        console.log('üé§ Resuming recording after dance');
        this.audioRecorder.startBargeInMonitoring();
    }
}
```

**Critical Details:**
- Audio playback NOT stopped - Gemini continues speaking
- Recording NOT stopped - user can interrupt
- Error recovery stops dance mode cleanly
- wasRecording tracked to resume properly
- Volume clamped to 0-1 range for safety

---

### Step 5: Configure Dance Mode

**File:** `frontend/config.json`

```json
{
  "danceMode": {
    "enabled": true,
    "musicFile": {
      "local": "media/music/dance.mp3",
      "cloud": "https://storage.googleapis.com/avatar-478217-videos/music/dance.mp3"
    },
    "duration": 10084,
    "volume": 0.3,
    "_comment": "Duration matches video exactly (10.084s) with 1s fade out. Plays background music at specified volume while Gemini speaks."
  }
}
```

**Video Configuration:**
```json
{
  "videos": {
    "local": {
      "dancing": "media/video/dance.webm"
    },
    "cloud": {
      "dancing": "https://storage.googleapis.com/avatar-478217-videos/video/dance.webm"
    }
  }
}
```

---

### Step 6: Audio/Video Synchronization

**Measure Video Duration:**
```bash
ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 frontend/media/video/dance.webm
# Output: 10.084000
```

**Edit Audio to Match:**
```bash
# Cut audio to exact duration with 1-second fade out
ffmpeg -i frontend/media/music/dance.mp3 \
  -t 10.084 \
  -af "afade=t=out:st=9.084:d=1" \
  -y frontend/media/music/dance_edited.mp3

# Verify duration
ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 frontend/media/music/dance_edited.mp3
# Output: 10.084331

# Replace original
mv frontend/media/music/dance.mp3 frontend/media/music/dance_backup.mp3
mv frontend/media/music/dance_edited.mp3 frontend/media/music/dance.mp3
```

**Upload to Cloud Storage:**
```bash
# Upload edited audio
gcloud storage cp frontend/media/music/dance.mp3 gs://avatar-478217-videos/music/dance.mp3

# Ensure public access
gcloud storage buckets add-iam-policy-binding gs://avatar-478217-videos \
  --member=allUsers \
  --role=roles/storage.objectViewer

# Verify upload
gcloud storage ls -L gs://avatar-478217-videos/music/dance.mp3
```

**Update Config Duration:**
```json
{
  "danceMode": {
    "duration": 10084  // Matches video/audio exactly (10.084s in milliseconds)
  }
}
```

---

### Step 7: Deployment

**Backend Deployment:**
```bash
cd backend
gcloud run deploy gemini-avatar-backend \
  --source . \
  --region us-central1 \
  --allow-unauthenticated \
  --min-instances 1 \
  --max-instances 10 \
  --timeout 300 \
  --memory 512Mi \
  --cpu 1 \
  --set-env-vars BACKEND_HOST=0.0.0.0,BACKEND_PORT=8080,DEBUG=false,REQUIRE_AUTH=false,FIREBASE_PROJECT_ID=avatar-478217 \
  --set-secrets GEMINI_API_KEY=GEMINI_API_KEY:latest \
  --port 8080
```

**Frontend Deployment:**
```bash
# Update cache version to force browser refresh
# Edit frontend/index.html line ~1116:
<script type="module" src="app.js?v=TOOLCALL_COMPLETE"></script>

firebase deploy --only hosting
```

---

### Common Issues & Solutions

**Issue #1: Tool call received but attribute error**
```
AttributeError: 'LiveServerToolCall' object has no attribute 'function_call'
```
**Solution:** SDK uses `function_calls` (plural), not `function_call` (singular)

**Issue #2: Dance triggers multiple times**
**Solution:** Implement debouncing with 2-second cooldown in backend

**Issue #3: Gemini goes silent during dance**
**Solution:** Update function description to explicitly require concurrent speech

**Issue #4: User can't interrupt dance**
**Solution:** Keep recording active - don't stop barge-in monitoring

**Issue #5: Audio/video desync**
**Solution:** Match audio duration exactly to video duration with fade out

**Issue #6: Dance music too loud**
**Solution:** Set volume to 0.3 (30%) so Gemini's voice is clear

**Issue #7: Dual triggering (client + server)**
**Solution:** Remove all client-side keyword detection, use only server-side function calling

---

### Testing Checklist

- [ ] User says "dance" ‚Üí Gemini calls function
- [ ] Backend logs show tool_call received with correct ID
- [ ] Frontend receives tool_call message
- [ ] Dance video plays for exact duration
- [ ] Music plays at 30% volume with fade out
- [ ] Gemini continues speaking during dance
- [ ] User can interrupt by speaking
- [ ] Dance ends automatically after 10.084s
- [ ] Returns to listening state correctly
- [ ] No duplicate triggers within 2 seconds
- [ ] Error recovery works if music fails to load

---

### Template for New Interactive Features

To add a new function-calling feature (e.g., "applause mode", "spotlight effect"):

1. **Define function** in `backend/config/gemini_config.py` ‚Üí tools array
2. **Add backstory rule** in `backend/whinny_backstory.json` ‚Üí reaction_imperatives
3. **Implement backend handler** in `backend/core/websocket_handler.py` ‚Üí tool_call processing
4. **Add session state** in `backend/core/session.py` ‚Üí debouncing fields
5. **Implement frontend handler** in `frontend/app.js` ‚Üí tool_call switch case
6. **Add configuration** in `frontend/config.json` ‚Üí new feature config
7. **Prepare media** - edit/compress audio/video to match durations
8. **Upload to Cloud Storage** - make publicly accessible
9. **Update cache version** in `frontend/index.html`
10. **Deploy both** backend (Cloud Run) and frontend (Firebase Hosting)
11. **Test thoroughly** - all scenarios including errors

**Key Principles:**
- Server-side detection only (no client-side keywords)
- Always send tool_response back to Gemini
- Implement debouncing for duplicate calls
- Add comprehensive error handling
- Log with IDs and timestamps for debugging
- Keep audio/recording active unless explicitly required to stop
- Match media durations precisely
- Make volume configurable
- Provide user feedback (log messages)
